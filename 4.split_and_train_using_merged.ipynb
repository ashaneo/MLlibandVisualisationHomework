{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# train_lyrics_classifier.py\n",
    "#\n",
    "# Spark-ML pipeline for 7-genre lyric classification\n",
    "# -- requires Spark 3.x + Python 3.8+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Task                                                                             | Notes                                                                                                      |\n",
    "| -------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n",
    "| Install **Java 11**, **Spark 3.5 +**, **Hadoop winutils**, and **Python 3.10 +** | Verify `spark-shell` works from **any** directory (add `%SPARK_HOME%\\bin` to `PATH`).                      |\n",
    "| Create a fresh virtual-env                                                       | `python -m venv mlenv && mlenv\\Scripts\\activate && pip install pyspark==3.5.0 pandas matplotlib streamlit` |\n",
    "| Create a Git repo just for the homework                                          | Makes ZIP assembly painless.                                                                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, trim, regexp_replace, length\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pathlib, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Lyrics7Genres\").getOrCreate()\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Paths & config  ───────────────────────────────────────────────────────────\n",
    "MEND_CSV  = \"data/Merged_dataset.csv\"   # change if needed\n",
    "MODEL_DIR = \"model_stage3_merged\"\n",
    "TRAIN_DIR = \"data/train80\"\n",
    "TEST_DIR  = \"data/test20\"\n",
    "SEED      = 42\n",
    "os.makedirs(\"data\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+-----+--------------------------------------------------------------------------------+\n",
      "|         artist_name|          track_name|release_date|genre|                                                                          lyrics|\n",
      "+--------------------+--------------------+------------+-----+--------------------------------------------------------------------------------+\n",
      "|              mukesh|mohabbat bhi jhoothi|        1950|  pop|hold time feel break feel untrue convince speak voice tear try hold hurt try ...|\n",
      "|       frankie laine|           i believe|        1950|  pop|believe drop rain fall grow believe darkest night candle glow believe go astr...|\n",
      "|         johnnie ray|                 cry|        1950|  pop|sweetheart send letter goodbye secret feel better wake dream think real false...|\n",
      "|         pérez prado|            patricia|        1950|  pop|kiss lips want stroll charm mambo chacha meringue heaven arm japan brag geish...|\n",
      "|giorgos papadopoulos|  apopse eida oneiro|        1950|  pop|till darling till matter know till dream live apart know hearts till world fr...|\n",
      "+--------------------+--------------------+------------+-----+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Load & preview dataset  ───────────────────────────────────────────────────\n",
    "# Reads the CSV into a Spark DataFrame.\n",
    "df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"multiLine\", \"true\")   # preserves line breaks in lyrics\n",
    "         .csv(MEND_CSV)\n",
    ")\n",
    "df.show(5, truncate=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lyrics ─▶ tokens ─▶ nostop ─▶ HashingTF ─▶ tf  (term-frequency vector)\n",
    "                                                   │\n",
    "                                                   ▼\n",
    "                                                 IDF\n",
    "                                                   │\n",
    "                                                   ▼\n",
    "                                               features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean rows: 28522\n"
     ]
    }
   ],
   "source": [
    "# 4. Minimal cleaning  ─────────────────────────────────────────────────────────\n",
    "df = (\n",
    "    df.select(\"artist_name\", \"track_name\", \"release_date\", \"genre\", \"lyrics\")  #Keeps only the five columns required in the assignment.\n",
    "      .withColumn(\"genre\", trim(lower(col(\"genre\"))))\n",
    "      .withColumn(\"lyrics\", regexp_replace(col(\"lyrics\"), r\"\\s+\", \" \"))\n",
    "      .filter(length(col(\"lyrics\")) > 0)\n",
    "      .filter(col(\"release_date\").rlike(r\"^\\d{4}$\"))     # keep 4-digit year\n",
    ")\n",
    "print(\"Clean rows:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 22846  Test: 5676\n"
     ]
    }
   ],
   "source": [
    "# 5. 80 / 20 train-test split  ────────────────────────────────────────────────\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=SEED)\n",
    "train_df.write.mode(\"overwrite\").parquet(TRAIN_DIR)\n",
    "test_df.write.mode(\"overwrite\").parquet(TEST_DIR)\n",
    "print(\"Train:\", train_df.count(), \" Test:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"lyrics\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "stop_rm   = StopWordsRemover(inputCol=\"tokens\", outputCol=\"nostop\")\n",
    "tf        = HashingTF(inputCol=\"nostop\", outputCol=\"tf\", numFeatures=1 << 18)\n",
    "idf       = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
    "lab       = StringIndexer(inputCol=\"genre\", outputCol=\"label\") # The StringIndexer scans every distinct genre, assigns each one an integer ID (0-based), and writes those integers into a new column called label.\n",
    "clf       = LogisticRegression(maxIter=30, regParam=0.3, elasticNetParam=0.1)  # uses 'features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Stage                  | What it does                                             | Key params                                                   |\n",
    "| ---------------------- | -------------------------------------------------------- | ------------------------------------------------------------ |\n",
    "| **RegexTokenizer**     | Splits text on non-word chars to produce tokens.         | `pattern=\"\\\\W\"` (= any char that’s *not* a letter/digit).    |\n",
    "| **StopWordsRemover**   | Drops English stop words (“the”, “and”…).                | Default list; you can tweak for music jargon.                |\n",
    "| **HashingTF**          | Maps tokens → sparse term-frequency vectors via hashing. | `numFeatures=2¹⁸` (≈ 260 K dims) limits collisions.          |\n",
    "| **IDF**                | Re-weights TF vectors by inverse-document frequency.     | `outputCol=\"features\"` so the classifier can find it.        |\n",
    "| **StringIndexer**      | Converts genre strings → numeric labels 0–6.             | Needed for any Spark classifier.                             |\n",
    "| **LogisticRegression** | Multi-class (One-Vs-Rest) linear model.                  | `regParam` + `elasticNetParam` control L2/L1 regularisation. |\n",
    "| **Pipeline**           | Chains everything so you fit+transform in one shot.      | —                                                            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train model  ──────────────────────────────────────────────────────────────\n",
    "pipe = Pipeline(stages=[tokenizer, stop_rm, tf, idf, lab, clf])\n",
    "model = pipe.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.2419\n"
     ]
    }
   ],
   "source": [
    "# 8. Evaluate  ─────────────────────────────────────────────────────────────────\n",
    "preds = model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "acc = evaluator.evaluate(preds)\n",
    "print(f\"Test accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to C:\\Users\\DELL\\Desktop\\Sem 8\\1. Big Data Analytics\\MLlibAssignment\\submission1\\model_stage3_merged\n"
     ]
    }
   ],
   "source": [
    "# 9. Save artefacts  ───────────────────────────────────────────────────────────\n",
    "model.write().overwrite().save(MODEL_DIR)\n",
    "print(\"Model saved to\", pathlib.Path(MODEL_DIR).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Stop Spark  ──────────────────────────────────────────────────────────────\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taras’s method in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, trim, regexp_replace, length\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pathlib, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Lyrics7Genres\").getOrCreate()\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Paths & config  ───────────────────────────────────────────────────────────\n",
    "MEND_CSV  = \"data/Merged_dataset.csv\"   # change if needed\n",
    "TRAIN_DIR = \"data/train80\"\n",
    "TEST_DIR  = \"data/test20\"\n",
    "SEED      = 42\n",
    "os.makedirs(\"data\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+-----+--------------------------------------------------------------------------------+\n",
      "|         artist_name|          track_name|release_date|genre|                                                                          lyrics|\n",
      "+--------------------+--------------------+------------+-----+--------------------------------------------------------------------------------+\n",
      "|              mukesh|mohabbat bhi jhoothi|        1950|  pop|hold time feel break feel untrue convince speak voice tear try hold hurt try ...|\n",
      "|       frankie laine|           i believe|        1950|  pop|believe drop rain fall grow believe darkest night candle glow believe go astr...|\n",
      "|         johnnie ray|                 cry|        1950|  pop|sweetheart send letter goodbye secret feel better wake dream think real false...|\n",
      "|         pérez prado|            patricia|        1950|  pop|kiss lips want stroll charm mambo chacha meringue heaven arm japan brag geish...|\n",
      "|giorgos papadopoulos|  apopse eida oneiro|        1950|  pop|till darling till matter know till dream live apart know hearts till world fr...|\n",
      "+--------------------+--------------------+------------+-----+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Load & preview dataset  ───────────────────────────────────────────────────\n",
    "# Reads the CSV into a Spark DataFrame.\n",
    "df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"multiLine\", \"true\")   # preserves line breaks in lyrics\n",
    "         .csv(MEND_CSV)\n",
    ")\n",
    "df.show(5, truncate=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean rows: 28522\n"
     ]
    }
   ],
   "source": [
    "# 4. Minimal cleaning  ─────────────────────────────────────────────────────────\n",
    "df = (\n",
    "    df.select(\"artist_name\", \"track_name\", \"release_date\", \"genre\", \"lyrics\")  #Keeps only the five columns required in the assignment.\n",
    "      .withColumn(\"genre\", trim(lower(col(\"genre\"))))\n",
    "      .withColumn(\"lyrics\", regexp_replace(col(\"lyrics\"), r\"\\s+\", \" \"))\n",
    "      .filter(length(col(\"lyrics\")) > 0)\n",
    "      .filter(col(\"release_date\").rlike(r\"^\\d{4}$\"))     # keep 4-digit year\n",
    ")\n",
    "print(\"Clean rows:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 22846  Test: 5676\n"
     ]
    }
   ],
   "source": [
    "# 5. 80 / 20 train-test split  ────────────────────────────────────────────────\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=SEED)\n",
    "train_df.write.mode(\"overwrite\").parquet(TRAIN_DIR)\n",
    "test_df.write.mode(\"overwrite\").parquet(TEST_DIR)\n",
    "print(\"Train:\", train_df.count(), \" Test:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build feature pipeline  (same as before, up to IDF) -----------------------\n",
    "tokenizer = RegexTokenizer(inputCol=\"lyrics\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "stop_rm   = StopWordsRemover(inputCol=\"tokens\", outputCol=\"nostop\")\n",
    "tf        = HashingTF(inputCol=\"nostop\", outputCol=\"tf\", numFeatures=1 << 18)\n",
    "idf       = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
    "lab       = StringIndexer(inputCol=\"genre\", outputCol=\"label\")\n",
    "\n",
    "# 6b. Classifier + param grid --------------------------------------------------\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(\n",
    "        maxIter=100,\n",
    "        elasticNetParam=1.0,      # pure L1 like Taras\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\"\n",
    ")\n",
    "\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "      .addGrid(lr.regParam, [0.01, 0.05, 0.1, 0.3])\n",
    "      .build()\n",
    ")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "        estimator=lr,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        trainRatio=0.8,           # 80 % of train_df used for fitting,\n",
    "        seed=SEED                 # 20 % for validation (like his 70/30)\n",
    ")\n",
    "\n",
    "pipe = Pipeline(stages=[tokenizer, stop_rm, tf, idf, lab, tvs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regParam: 0.01\n"
     ]
    }
   ],
   "source": [
    "# 7. Train model (unchanged call, but now includes tuning) ---------------------\n",
    "model = pipe.fit(train_df)\n",
    "\n",
    "best_lr = model.stages[-1].bestModel\n",
    "print(\"Best regParam:\", best_lr._java_obj.getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          accuracy: 0.3129\n",
      "                f1: 0.2490\n",
      " weightedPrecision: 0.4631\n",
      "    weightedRecall: 0.3129\n"
     ]
    }
   ],
   "source": [
    "# 8. Evaluate on the held-out test set -----------------------------------------\n",
    "preds = model.transform(test_df)\n",
    "for m in [\"accuracy\", \"f1\", \"weightedPrecision\", \"weightedRecall\"]:\n",
    "    val = evaluator.setMetricName(m).evaluate(preds)\n",
    "    print(f\"{m:>18}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver python : c:\\Users\\DELL\\anaconda3\\envs\\music-genre\\python.exe\n",
      "Worker python : c:\\Users\\DELL\\anaconda3\\envs\\music-genre\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "# absolute path of the conda-env python that Jupyter is using now\n",
    "PY = sys.executable                        # e.g. C:\\Users\\DELL\\anaconda3\\envs\\music-genre\\python.exe\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]        = PY   # workers\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PY   # driver  (needed mainly when you use spark-submit)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Lyrics7Genres\")\n",
    "         .config(\"spark.pyspark.python\", PY)\n",
    "         .config(\"spark.pyspark.driver.python\", PY)\n",
    "         .getOrCreate())\n",
    "\n",
    "print(\"Driver python :\", sys.executable)\n",
    "print(\"Worker python :\", spark.sparkContext.pythonExec)   # should match now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Model saved to: C:\\Users\\DELL\\Desktop\\Sem 8\\1. Big Data Analytics\\MLlibAssignment\\submission1\\model_stage4_merged_Trans_way_new\n"
     ]
    }
   ],
   "source": [
    "import pathlib, shutil\n",
    "\n",
    "# 1️⃣  Path:  `<notebook directory>/model_stage2`\n",
    "save_dir = pathlib.Path.cwd() / \"model_stage4_merged_Trans_way_new\"\n",
    "\n",
    "# 2️⃣  Remove any previous run so .overwrite() won’t clash with a *file*\n",
    "if save_dir.exists():\n",
    "    shutil.rmtree(save_dir)\n",
    "\n",
    "# 3️⃣  Persist the fitted pipeline\n",
    "# Spark is happy with either a plain absolute path or a file:// URI.\n",
    "# We'll use the plain path to keep it readable.\n",
    "model.write().overwrite().save(str(save_dir))\n",
    "\n",
    "print(\"✅  Model saved to:\", save_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload OK, stages: 6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "reloaded = PipelineModel.load(f\"model_stage2_Trans_way\")\n",
    "print(\"Reload OK, stages:\", len(reloaded.stages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-genre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
